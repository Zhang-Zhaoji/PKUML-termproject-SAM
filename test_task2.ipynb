{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai.transforms as mt\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import ignite\n",
    "import numpy as np\n",
    "import torch\n",
    "import monai\n",
    "import torchvision.transforms as transform\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # remove some scikit-image warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_label(Ground_truth_mask,prompt = \"box\"):\n",
    "    label_images = []\n",
    "    prompt_images = []\n",
    "    local_gt = Ground_truth_mask\n",
    "    for i in range(1,int(np.max(local_gt))+1):\n",
    "        i_gt_mask = np.where(local_gt == i, 1, 0)\n",
    "        if np.sum(i_gt_mask.flatten()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            label_images.append(i_gt_mask)\n",
    "            if prompt == \"point\":\n",
    "                # 随机选一个点\n",
    "                point = np.zeros((1,2))\n",
    "                indices = np.nonzero(i_gt_mask)\n",
    "                random_index = np.random.randint(0,len(indices[0])-1)\n",
    "                point[0,0] ,point[0,1] = indices[2][random_index],indices[1][random_index]\n",
    "                prompt_images.append(point)\n",
    "                #SAM_mask , scores, logits = predictor.predict(point_coords=point,point_labels=np.array([1]),multimask_output=False,)\n",
    "            elif prompt == \"points\":\n",
    "                # 随机选5个点\n",
    "                indices = np.nonzero(i_gt_mask)\n",
    "                random_index = np.random.random_integers(0,len(indices[0])-1,5)\n",
    "                points = np.zeros((len(random_index),2))\n",
    "                for j in range(len(random_index)):\n",
    "                    points[j,0], points[j,1] = indices[2][random_index[j]],indices[1][random_index[j]]\n",
    "                prompt_images.append(points)\n",
    "                #SAM_mask , scores, logits = predictor.predict(point_coords=points,point_labels=np.ones(len(points)),multimask_output=False,)\n",
    "            elif prompt == \"box\":\n",
    "                indices = np.nonzero(i_gt_mask)\n",
    "                x_min = min(indices[2])\n",
    "                x_max = max(indices[2])\n",
    "                y_min = min(indices[1])\n",
    "                y_max = max(indices[1])\n",
    "                input_box = np.array([x_min,y_min,x_max,y_max])\n",
    "                prompt_images.append(input_box)\n",
    "                #SAM_mask , scores, logits = predictor.predict(point_coords=None,point_labels=None,box=input_box,multimask_output=False,)\n",
    "            else:\n",
    "                raise NameError(\"prompt should be in [\\\"point\\\",\\\"points\\\",\\\"box\\\"]\")\n",
    "            \n",
    "    return label_images, prompt_images\n",
    "\n",
    "def Gray2RGB(image):\n",
    "    # 进来的是 batchsize = 1 * 512 * 512\n",
    "    # 返回一个 B*C*H*W\n",
    "    input_img = torch.zeros((image.shape[1],image.shape[2],3))\n",
    "    input_img[:,:,0] = input_img[:,:,1]  = input_img[:,:,2]  = image[0,:,:] #(image-torch.min(image))/(torch.max(image))*255\n",
    "    input_img = np.uint8(input_img)\n",
    "    #input_img = input_img.to(device=\"cuda\")\n",
    "    return input_img\n",
    "\n",
    "def Extra_Dim(image):\n",
    "    image_4d = np.zeros((1,3,image.shape[0],image.shape[1]))\n",
    "    image_4d[0,0,:,:] = image[:,:,0]\n",
    "    image_4d[0,1,:,:] = image[:,:,1]\n",
    "    image_4d[0,2,:,:] = image[:,:,2]\n",
    "    return image_4d\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "def image_preprocess(image,sam_model,device):\n",
    "    image = Gray2RGB(image)\n",
    "    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    input_image = sam_model.preprocess(transformed_image)\n",
    "    original_image_size = image.shape[:2]\n",
    "    input_size = tuple(transformed_image.shape[-2:])\n",
    "    return input_image, original_image_size, input_size\n",
    "\n",
    "def Dice(SAM_mask,i_gt_mask):\n",
    "    output_mat = SAM_mask[0,0,:,:] * i_gt_mask[0,0,:,:]\n",
    "    overlap = torch.sum(output_mat)\n",
    "    return 2* overlap / (torch.sum(SAM_mask)+torch.sum(i_gt_mask))\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.apps import datasets\n",
    "import json\n",
    "import os\n",
    "from monai import transforms\n",
    "import torchvision\n",
    "from monai.transforms import LoadImaged,EnsureChannelFirstd,Compose,ToTensord\n",
    "import logging\n",
    "\n",
    "image_transform = Compose([\n",
    "    LoadImaged(keys=(\"image\", \"label\")),\n",
    "#    torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "    ToTensord(keys=(\"image\", \"label\"))\n",
    "])\n",
    "\n",
    "with open(\"./validation_data.json\") as file1:\n",
    "    dataset = json.load(file1)\n",
    "\n",
    "#train_dataset =  monai.data.CacheDataset(dataset[\"training\"], transform=image_transform)\n",
    "train_dataset =  monai.data.Dataset(dataset, transform=image_transform)\n",
    "train_loader  = monai.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry \n",
    "import torch \n",
    "import os \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 根据检查点加载模型\n",
    "#device = \"cpu\"\n",
    "sam_model = sam_model_registry[\"vit_b\"](checkpoint=\"./checkpoint/sam_vit_b_01ec64.pth\")\n",
    "#sam_model.train()\n",
    "predictor = SamPredictor(sam_model)\n",
    "for name, param in sam_model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "sam_model.to(device=device)\n",
    "# 定义损失函数和优化器\n",
    "# hyperparameters\n",
    "max_epochs = 1\n",
    "#lr = 5e-6\n",
    "#wd = 0\n",
    "#optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 177/408 [18:39<23:57,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0,processf177\n",
      "Mean loss: 0.6017636007097902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 354/408 [37:17<05:45,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0,processf354\n",
      "Mean loss: 0.6110166907652497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [43:04<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize\n",
    "from segment_anything import SamPredictor, sam_model_registry, utils\n",
    "losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_losses = []\n",
    "    batch_number = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_number += 1\n",
    "    # forward pass\n",
    "    # batch image : [batch_size * 512 * 512], label : [batch_size * 512 * 512]\n",
    "        loss = 0\n",
    "        ori_input_image, input_label = batch[\"image\"],batch[\"label\"]\n",
    "        input_image, original_image_size, input_size = image_preprocess(ori_input_image,sam_model,device)\n",
    "        label_images, prompt_images = prompt_label(input_label)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam_model.image_encoder(input_image)\n",
    "        for k in range(len(label_images)):\n",
    "            box = ResizeLongestSide(sam_model.image_encoder.img_size).apply_boxes(prompt_images[k], original_image_size)\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            box_torch = box_torch[None, :]\n",
    "            with torch.no_grad():\n",
    "                sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(points=None,boxes = box_torch, masks=None)\n",
    "        # compute loss\n",
    "            low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "            )\n",
    "            upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "            binary_mask = F.normalize(F.threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "            gt_mask_resized = torch.from_numpy(np.resize(label_images[k], (1, 1, label_images[k].shape[1], label_images[k].shape[2]))).to(device)\n",
    "            gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "            #print(torch.sum(binary_mask))\n",
    "            #print(torch.sum(gt_binary_mask))\n",
    "            loss += Dice(binary_mask, gt_binary_mask)\n",
    "\n",
    "            \"\"\" fig, ax = plt.subplots(1,1)\n",
    "            plt.imshow(ori_input_image[0,:,:])\n",
    "            show_mask(binary_mask[0,0,:,:].detach().numpy(),ax=ax)\n",
    "            show_mask(gt_binary_mask[0,0,:,:].detach().numpy(),ax=ax)\n",
    "            print(box_torch)\n",
    "            print(prompt_images[k])\n",
    "            show_box(prompt_images[k], ax)\n",
    "            plt.title(f\" Score: {loss:.3f}\", fontsize=18)\n",
    "            plt.show()\"\"\"\n",
    "        loss /= len(label_images)\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        losses.append(epoch_losses)\n",
    "        if batch_number % (1770//10) == 0:\n",
    "            print(f'EPOCH: {epoch},processf{batch_number}')\n",
    "            print(f'Mean loss: {mean(epoch_losses)}')\n",
    "        torch.cuda.empty_cache()\n",
    "    #PATH = f\"finetune/fine_tuned_sam_{4+epoch}.pth\"\n",
    "    #torch.save(sam_model.state_dict(), PATH)\n",
    "json_data = json.dumps(epoch_losses,indent=4,separators=(',', ': '))\n",
    "\n",
    "with open('box_original_mDice.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 177/408 [17:49<22:50,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0,processf177\n",
      "Mean loss: 0.10263380439314691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 354/408 [35:10<05:24,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0,processf354\n",
      "Mean loss: 0.0973900854333513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [40:29<00:00,  5.95s/it]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_losses = []\n",
    "    batch_number = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_number += 1\n",
    "    # forward pass\n",
    "    # batch image : [batch_size * 512 * 512], label : [batch_size * 512 * 512]\n",
    "        loss = 0\n",
    "        input_image, input_label = batch[\"image\"],batch[\"label\"]\n",
    "        input_image, original_image_size, input_size = image_preprocess(input_image,sam_model,device)\n",
    "        label_images, prompt_images = prompt_label(input_label,prompt=\"point\")\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam_model.image_encoder(input_image)\n",
    "        for k in range(len(label_images)):\n",
    "            point = ResizeLongestSide(sam_model.image_encoder.img_size).apply_coords(prompt_images[k], original_image_size)\n",
    "            point_torch = torch.as_tensor(point, dtype=torch.float, device=device)\n",
    "            point_label = torch.ones(len(point_torch))\n",
    "            point_torch = point_torch[None, :]\n",
    "            point_label = point_label[None,:]\n",
    "            with torch.no_grad():\n",
    "               sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(points=(point_torch,point_label),boxes = None, masks=None)\n",
    "        # compute loss\n",
    "            low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "            )\n",
    "            upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "            binary_mask = F.normalize(F.threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "            gt_mask_resized = torch.from_numpy(np.resize(label_images[0], (1, 1, label_images[k].shape[1], label_images[k].shape[2]))).to(device)\n",
    "            gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "            loss += Dice(binary_mask, gt_binary_mask)\n",
    "        loss /= len(label_images)\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        losses.append(epoch_losses)\n",
    "        if batch_number % (1770//10) == 0:\n",
    "            print(f'EPOCH: {epoch},processf{batch_number}')\n",
    "            print(f'Mean loss: {mean(epoch_losses)}')\n",
    "    #PATH = f\"finetune/fine_tuned_sam_{4+epoch}.pth\"\n",
    "    #torch.save(sam_model.state_dict(), PATH)\n",
    "json_data = json.dumps(epoch_losses,indent=4,separators=(',', ': '))\n",
    "\n",
    "with open('point_original_mDice.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 20/408 [02:11<42:29,  6.57s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28260\\2218732627.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlabel_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompt_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"points\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mimage_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msam_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResizeLongestSide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msam_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_coords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_image_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1512\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1514\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1512\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1514\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_hw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow_partition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;31m# Reverse window partition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1512\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1514\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_rel_pos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_decomposed_rel_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrel_pos_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrel_pos_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py\u001b[0m in \u001b[0;36madd_decomposed_rel_pos\u001b[1;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mq_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[0mk_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m     \u001b[0mRh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rel_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel_pos_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m     \u001b[0mRw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rel_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel_pos_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\10690\\anaconda3\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py\u001b[0m in \u001b[0;36mget_rel_pos\u001b[1;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mrelative_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mq_coords\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mk_coords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrel_pos_resized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrelative_coords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_losses = []\n",
    "    batch_number = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_number += 1\n",
    "    # forward pass\n",
    "    # batch image : [batch_size * 512 * 512], label : [batch_size * 512 * 512]\n",
    "        loss = 0\n",
    "        input_image, input_label = batch[\"image\"],batch[\"label\"]\n",
    "        input_image, original_image_size, input_size = image_preprocess(input_image,sam_model,device)\n",
    "        label_images, prompt_images = prompt_label(input_label,prompt=\"points\")\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam_model.image_encoder(input_image)\n",
    "        for k in range(len(label_images)):\n",
    "            point = ResizeLongestSide(sam_model.image_encoder.img_size).apply_coords(prompt_images[0], original_image_size)\n",
    "            point_torch = torch.as_tensor(point, dtype=torch.float, device=device)\n",
    "            point_label = torch.ones(len(point_torch))\n",
    "            point_torch = point_torch[None, :]\n",
    "            point_label = point_label[None,:]\n",
    "            with torch.no_grad():\n",
    "                sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(points=(point_torch,point_label),boxes = None, masks=None)\n",
    "        # compute loss\n",
    "            low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "            )\n",
    "            upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "            binary_mask = F.normalize(F.threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "            gt_mask_resized = torch.from_numpy(np.resize(label_images[0], (1, 1, label_images[k].shape[1], label_images[k].shape[2]))).to(device)\n",
    "            gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "            loss += Dice(binary_mask, gt_binary_mask)\n",
    "        loss /= len(label_images)\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        losses.append(epoch_losses)\n",
    "        if batch_number % (1770//10) == 0:\n",
    "            print(f'EPOCH: {epoch},processf{batch_number}')\n",
    "            print(f'Mean loss: {mean(epoch_losses)}')\n",
    "    #PATH = f\"finetune/fine_tuned_sam_{4+epoch}.pth\"\n",
    "    #torch.save(sam_model.state_dict(), PATH)\n",
    "json_data = json.dumps(epoch_losses,indent=4,separators=(',', ': '))\n",
    "\n",
    "with open('points_finetune_mDice.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
